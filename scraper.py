"""
JobHunter v5 Scraper — Multi-source, zero-cost job aggregation.

Sources (in order of call during a scrape):
  1. The Muse       — no key, 500 req/hr unauthenticated. Tech/startup focus.
  2. Remotive       — no key, generous limits. Remote-only tech jobs.
  3. Greenhouse     — no key, not rate-limited (CDN-cached). Direct company boards.
  4. USAJobs        — free API key (email signup only). Federal/government jobs.
  5. JSearch        — 200 req/month free. Reserved for targeted company-specific searches.

Dynamic search strategy:
  - Job titles, Greenhouse company boards, and JSearch queries are ALL generated by AI
    from the user's resume + context on first scrape, then cached per user in the DB.
  - Fallback hardcoded lists are used only if AI generation fails or no resume exists.
  - This means every user gets a personalized scrape, not a one-size-fits-all list.

Rate-limiting strategy:
  - Hard sleep between every request (configurable per source)
  - Check response headers for rate-limit signals and back off automatically
  - Sources that return 429 are skipped gracefully — rest of scrape continues
  - JSearch budget guard: skipped automatically if <5 calls remain this month
"""

import requests
import json
import time
import re
from datetime import datetime


# ─── TITLE PRE-FILTER ─────────────────────────────────────────────────────────

# These are roles that are always irrelevant regardless of resume:
# senior/executive titles and completely off-field roles.
EXCLUDE_TITLE_KEYWORDS = [
    "senior", "sr.", " sr ", "staff ", "principal", "director", "vp ", "vice president",
    "manager", "head of", "lead ", " lead", "architect", "cto", "cso", "chief",
    "surgeon", "physician", "nurse", "dental", "attorney", "lawyer",
    "account executive", "truck driver", "cdl", "warehouse", "hvac",
    "plumber", "electrician", "carpenter", "welder", "forklift",
]


def is_relevant_title(title: str) -> bool:
    t = title.lower()
    return not any(kw in t for kw in EXCLUDE_TITLE_KEYWORDS)


def dedup_by_title_company(jobs: list) -> list:
    seen = set()
    result = []
    for job in jobs:
        key = re.sub(r'[^a-z0-9]', '', (job.get("title", "") + job.get("company", "")).lower())
        if key not in seen:
            seen.add(key)
            result.append(job)
    return result


def _safe_get(url, params=None, headers=None, timeout=20, source=""):
    """HTTP GET with automatic 429 detection and backoff."""
    try:
        resp = requests.get(url, params=params, headers=headers, timeout=timeout)
        if resp.status_code == 429:
            retry_after = int(resp.headers.get("Retry-After", 60))
            raise RateLimitError(f"{source} rate limited — retry after {retry_after}s", retry_after)
        resp.raise_for_status()
        return resp
    except RateLimitError:
        raise
    except requests.exceptions.Timeout:
        raise Exception(f"{source} request timed out")
    except requests.exceptions.RequestException as e:
        raise Exception(f"{source} request failed: {e}")


class RateLimitError(Exception):
    def __init__(self, msg, retry_after=60):
        super().__init__(msg)
        self.retry_after = retry_after


# ==============================================================================
# AI-DRIVEN SEARCH PROFILE GENERATION
# ==============================================================================

# Fallback lists used ONLY when AI generation fails or no resume is available.
# Intentionally generic — AI-generated lists will always be better.

FALLBACK_MUSE_CATEGORIES = ["Software Engineer", "Data Science", "IT", "QA"]
FALLBACK_MUSE_LEVELS = ["entry level", "mid level"]
FALLBACK_REMOTIVE_CATEGORIES = ["software-dev", "data", "devops", "qa"]

FALLBACK_GREENHOUSE_BOARDS = [
    {"name": "Salesforce",  "token": "salesforce"},
    {"name": "Stripe",      "token": "stripe"},
    {"name": "Notion",      "token": "notion"},
    {"name": "Figma",       "token": "figma"},
]

FALLBACK_JSEARCH_QUERIES = [
    {"name": "General Entry Level Software", "query": "entry level software engineer"},
    {"name": "Junior Developer",             "query": "junior developer entry level"},
]

FALLBACK_USAJOBS_KEYWORDS = [
    "Software Engineer", "Data Analyst", "IT Specialist", "Systems Analyst",
]


def generate_search_profile(resume_text: str, ai_context: str,
                             locations: list, api_key: str,
                             api_url: str, model_name: str, log_fn) -> dict:
    """
    Ask the AI to generate a fully personalized search profile from the user's resume.

    Returns a dict with:
      - muse_categories: list of The Muse category strings
      - muse_levels: list of Muse level strings (entry/mid)
      - remotive_categories: list of Remotive category slugs
      - greenhouse_boards: list of {name, token} dicts
      - jsearch_queries: list of {name, query} dicts
      - usajobs_keywords: list of keyword strings
      - title_include_keywords: extra title fragments that count as relevant for this resume
      - title_exclude_extra: extra title fragments to filter out for this user

    This profile is cached in the DB per user and only regenerated when the resume changes.
    """
    if not api_key or not resume_text:
        log_fn("Search profile: using fallback (no AI key or resume)")
        return _fallback_profile()

    location_names = [loc.get("label") or f"{loc['city']}, {loc['state']}" for loc in locations]
    location_str = ", ".join(location_names) if location_names else "any location"

    prompt = f"""You are a job search expert. Analyze this resume and generate a personalized job search profile.

RESUME:
{resume_text[:3000]}

EXTRA CONTEXT: {ai_context or "None provided"}

TARGET LOCATIONS: {location_str} (plus remote)

Return ONLY valid JSON — no markdown, no text outside the JSON object.

{{
  "muse_categories": ["2-4 The Muse category names from: Software Engineer, Data Science, Data Analytics, IT, QA, Design, Product, DevOps/Sysadmin, Cybersecurity, Project Management, Finance, Marketing, Sales, Customer Success, Legal, HR, Operations, Writing/Editing, UX"],
  "muse_levels": ["entry level and/or mid level — only include 'senior level' if 5+ YOE"],
  "remotive_categories": ["2-4 slugs from: software-dev, data, devops, product, design, finance, legal, marketing, customer-support, sales, hr, qa, writing, backend, frontend, fullstack, mobile, ios, android, infosec, erp, finance, management, all-other"],
  "greenhouse_boards": [
    {{"name": "Company Name", "token": "greenhouse-board-slug"}},
    "10-20 companies that would hire someone with this resume. Mix of big tech and local employers near the target locations."
  ],
  "jsearch_queries": [
    {{"name": "Descriptive Label", "query": "specific search string optimized for this resume"}},
    "5-10 targeted queries for best-fit roles and companies"
  ],
  "usajobs_keywords": ["3-6 federal job title keywords if government work is relevant, else empty array []"],
  "title_include_keywords": ["extra tech-relevant title fragments specific to this resume, e.g. 'solutions architect' if they have cloud exp"],
  "title_exclude_extra": ["title fragments clearly off-field for this specific resume — be specific, not broad"]
}}

Rules:
- Match seniority to the resume experience level (new grad = entry level, 5+ YOE = mid ok)
- Greenhouse tokens are the slug from boards.greenhouse.io/TOKEN (e.g. 'google', 'stripe', 'airbnb')
- JSearch queries should be specific and targeted: "Python backend engineer Chicago" not just "engineer"
- If the resume shows strong specialization (ML, security, embedded, etc.) weight those heavily
- Include local companies near the target locations when you know their Greenhouse board
- usajobs_keywords can be an empty array if federal jobs are not relevant"""

    try:
        resp = requests.post(
            api_url,
            headers={"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"},
            json={
                "model": model_name,
                "messages": [
                    {"role": "system", "content": "You are a JSON-only API. Return only valid JSON."},
                    {"role": "user", "content": prompt}
                ],
                "stream": False
            },
            timeout=90
        )
        resp.raise_for_status()
        content = resp.json()["choices"][0]["message"]["content"].strip()
        content_clean = re.sub(r'^```(?:json)?\s*', '', content)
        content_clean = re.sub(r'\s*```$', '', content_clean).strip()
        obj_match = re.search(r'\{[\s\S]*\}', content_clean)
        if not obj_match:
            raise ValueError("No JSON object found in response")
        profile = json.loads(obj_match.group())

        # Fill in any missing keys with fallbacks
        fallback = _fallback_profile()
        for key in fallback:
            if key not in profile or not profile[key]:
                profile[key] = fallback[key]

        log_fn(
            f"Search profile generated: "
            f"{len(profile.get('greenhouse_boards', []))} Greenhouse boards, "
            f"{len(profile.get('jsearch_queries', []))} JSearch queries, "
            f"{len(profile.get('muse_categories', []))} Muse categories"
        )
        return profile

    except Exception as e:
        log_fn(f"Search profile generation failed ({e}) — using fallback")
        return _fallback_profile()


def _fallback_profile() -> dict:
    """Generic fallback profile when AI generation is unavailable."""
    return {
        "muse_categories": FALLBACK_MUSE_CATEGORIES,
        "muse_levels": FALLBACK_MUSE_LEVELS,
        "remotive_categories": FALLBACK_REMOTIVE_CATEGORIES,
        "greenhouse_boards": FALLBACK_GREENHOUSE_BOARDS,
        "jsearch_queries": FALLBACK_JSEARCH_QUERIES,
        "usajobs_keywords": FALLBACK_USAJOBS_KEYWORDS,
        "title_include_keywords": [],
        "title_exclude_extra": [],
    }


def is_relevant_title_for_profile(title: str, profile: dict) -> bool:
    """Extended title relevance check using the user's personalized profile."""
    t = title.lower()
    # Extra user-specific excludes
    for kw in profile.get("title_exclude_extra", []):
        if kw.lower() in t:
            return False
    return is_relevant_title(title)


# ==============================================================================
# SOURCE 1: THE MUSE  (no key required, 500 req/hr)
# ==============================================================================

MUSE_BASE = "https://www.themuse.com/api/public/jobs"


def scrape_muse(log_fn, profile: dict):
    """
    Fetch jobs from The Muse using the user's AI-generated categories and levels.
    No key needed. 500 req/hr — we stay well under.
    Returns (jobs, api_calls).
    """
    all_jobs = []
    seen_ids = set()
    api_calls = 0
    categories = profile.get("muse_categories", FALLBACK_MUSE_CATEGORIES)
    levels = profile.get("muse_levels", FALLBACK_MUSE_LEVELS)

    log_fn(f"The Muse: {len(categories)} categories × {len(levels)} levels...")

    for category in categories:
        for level in levels:
            for page in range(0, 3):
                try:
                    resp = _safe_get(
                        MUSE_BASE,
                        params={"category": category, "level": level, "page": page, "descending": "true"},
                        timeout=15,
                        source="Muse"
                    )
                    api_calls += 1
                    data = resp.json()
                    results = data.get("results", [])
                    if not results:
                        break

                    new_count = 0
                    for job in results:
                        job_id = "muse_" + str(job.get("id", ""))
                        if job_id in seen_ids:
                            continue
                        title = (job.get("name") or "").strip()
                        if not title or not is_relevant_title_for_profile(title, profile):
                            continue
                        seen_ids.add(job_id)

                        company = job.get("company", {}).get("name", "")
                        locations_list = job.get("locations", [])
                        location_str = ", ".join(loc.get("name", "") for loc in locations_list) if locations_list else "Remote"

                        title_lower = title.lower()
                        loc_lower = location_str.lower()
                        if "remote" in title_lower or "remote" in loc_lower or not locations_list:
                            work_type = "Remote"
                        elif "hybrid" in title_lower or "hybrid" in loc_lower:
                            work_type = "Hybrid"
                        else:
                            work_type = "Onsite"

                        refs = job.get("refs", {})
                        apply_url = refs.get("landing_page", "")
                        contents = job.get("contents", "")

                        all_jobs.append({
                            "job_id": job_id,
                            "title": title,
                            "company": company,
                            "location": location_str,
                            "lat": None, "lng": None,
                            "work_type": work_type,
                            "salary_min": None, "salary_max": None, "salary_display": "",
                            "description": re.sub(r'<[^>]+>', '', contents)[:2500],
                            "apply_url": apply_url,
                            "company_url": apply_url,
                            "source": "The Muse",
                            "date_posted": job.get("publication_date", ""),
                        })
                        new_count += 1

                    log_fn(f"  Muse [{category} / {level}] p{page}: {new_count}")
                    time.sleep(0.5)
                    if len(results) < 20:
                        break

                except RateLimitError as e:
                    log_fn(f"  Muse rate limited — skipping {category}")
                    time.sleep(min(e.retry_after, 30))
                    break
                except Exception as e:
                    log_fn(f"  Muse error ({category}/{level}/p{page}): {e}")
                    break

    log_fn(f"The Muse: {len(all_jobs)} jobs, {api_calls} calls")
    return all_jobs, api_calls


# ==============================================================================
# SOURCE 2: REMOTIVE  (no key, remote-only, generous limits)
# ==============================================================================

REMOTIVE_BASE = "https://remotive.com/api/remote-jobs"


def scrape_remotive(log_fn, profile: dict):
    """
    Fetch remote jobs from Remotive using the user's AI-generated category list.
    Returns (jobs, api_calls).
    """
    all_jobs = []
    seen_ids = set()
    api_calls = 0
    categories = profile.get("remotive_categories", FALLBACK_REMOTIVE_CATEGORIES)

    log_fn(f"Remotive: {len(categories)} categories...")

    for category in categories:
        try:
            resp = _safe_get(REMOTIVE_BASE, params={"category": category, "limit": 100},
                             timeout=20, source="Remotive")
            api_calls += 1
            data = resp.json()

            new_count = 0
            for job in data.get("jobs", []):
                job_id = "rem_" + str(job.get("id", ""))
                if job_id in seen_ids:
                    continue
                title = (job.get("title") or "").strip()
                if not title or not is_relevant_title_for_profile(title, profile):
                    continue
                seen_ids.add(job_id)

                sal_str = job.get("salary", "") or ""
                sal_min, sal_max = _parse_salary_range(sal_str)
                candidate_loc = job.get("candidate_required_location", "") or "Worldwide"

                desc_html = job.get("description", "") or ""
                desc_text = re.sub(r'<[^>]+>', ' ', desc_html)
                desc_text = re.sub(r'\s+', ' ', desc_text).strip()[:2500]

                all_jobs.append({
                    "job_id": job_id,
                    "title": title,
                    "company": job.get("company_name", ""),
                    "location": f"Remote — {candidate_loc}",
                    "lat": None, "lng": None,
                    "work_type": "Remote",
                    "salary_min": sal_min, "salary_max": sal_max, "salary_display": sal_str,
                    "description": desc_text,
                    "apply_url": job.get("url", ""),
                    "company_url": job.get("url", ""),
                    "source": "Remotive",
                    "date_posted": job.get("publication_date", ""),
                })
                new_count += 1

            log_fn(f"  Remotive [{category}]: {new_count}")
            time.sleep(1.0)

        except RateLimitError as e:
            log_fn(f"  Remotive rate limited — skipping rest")
            time.sleep(min(e.retry_after, 30))
        except Exception as e:
            log_fn(f"  Remotive error ({category}): {e}")

    log_fn(f"Remotive: {len(all_jobs)} jobs, {api_calls} calls")
    return all_jobs, api_calls


# ==============================================================================
# SOURCE 3: GREENHOUSE COMPANY BOARDS  (no key, CDN-cached, not rate-limited)
# ==============================================================================

GREENHOUSE_API = "https://boards-api.greenhouse.io/v1/boards/{token}/jobs"


def scrape_greenhouse(log_fn, profile: dict):
    """
    Pull jobs from company Greenhouse boards.
    Board list comes from the user's AI-generated profile.
    Completely free, no auth, CDN-cached so not rate limited.
    Returns (jobs, api_calls).
    """
    boards = profile.get("greenhouse_boards", FALLBACK_GREENHOUSE_BOARDS)
    title_include = [kw.lower() for kw in profile.get("title_include_keywords", [])]

    all_jobs = []
    seen_ids = set()
    api_calls = 0
    failed_count = 0

    log_fn(f"Greenhouse: {len(boards)} company boards...")

    ENTRY_KEYWORDS = [
        "junior", "entry", "associate", "early career", "new grad", "graduate",
        "intern", "apprentice", " i ", " i)", "level 1", "level i", "jr.",
        " 1 ", "entry-level", "recent grad",
    ]

    TECH_KEYWORDS = [
        "software", "engineer", "developer", "data", "analyst",
        "cloud", "devops", "systems", "python", "java", "backend",
        "frontend", "full stack", "machine learning", "ai ", "ml ",
        "infrastructure", "platform", "site reliability", "sre",
        "quality", "qa", "test", "automation", "it ", "technology",
        "security", "cyber", "network", "database", "sql",
        "ios", "android", "mobile", "web", "api",
    ] + title_include

    for board in boards:
        try:
            url = GREENHOUSE_API.format(token=board["token"])
            resp = _safe_get(url, params={"content": "true"}, timeout=15,
                             source=f"Greenhouse/{board['name']}")
            api_calls += 1
            jobs = resp.json().get("jobs", [])

            new_count = 0
            for job in jobs:
                job_id = "gh_" + str(job.get("id", ""))
                if job_id in seen_ids:
                    continue
                title = (job.get("title") or "").strip()
                if not title:
                    continue

                title_lower = title.lower()
                has_entry = any(kw in title_lower for kw in ENTRY_KEYWORDS)
                has_tech = any(kw in title_lower for kw in TECH_KEYWORDS)

                if not has_tech:
                    continue
                if not has_entry and not is_relevant_title(title):
                    continue
                if not is_relevant_title_for_profile(title, profile):
                    continue

                seen_ids.add(job_id)

                loc = job.get("location", {})
                location_str = loc.get("name", "") if isinstance(loc, dict) else str(loc)
                loc_lower = location_str.lower()
                if "remote" in loc_lower or not location_str:
                    work_type = "Remote"
                elif "hybrid" in loc_lower:
                    work_type = "Hybrid"
                else:
                    work_type = "Onsite"

                content_html = job.get("content", "") or ""
                desc_text = re.sub(r'<[^>]+>', ' ', content_html)
                desc_text = re.sub(r'\s+', ' ', desc_text).strip()[:2500]

                all_jobs.append({
                    "job_id": job_id,
                    "title": title,
                    "company": board["name"],
                    "location": location_str,
                    "lat": None, "lng": None,
                    "work_type": work_type,
                    "salary_min": None, "salary_max": None, "salary_display": "",
                    "description": desc_text,
                    "apply_url": job.get("absolute_url", ""),
                    "company_url": f"https://boards.greenhouse.io/{board['token']}",
                    "source": "Greenhouse",
                    "date_posted": job.get("updated_at", ""),
                })
                new_count += 1

            if new_count:
                log_fn(f"  Greenhouse [{board['name']}]: {new_count}")
            time.sleep(0.3)

        except RateLimitError as e:
            log_fn(f"  Greenhouse [{board['name']}] rate limited — skipping")
            time.sleep(min(e.retry_after, 15))
        except Exception:
            failed_count += 1
            time.sleep(0.2)

    if failed_count:
        log_fn(f"  Greenhouse: {failed_count} boards not found (tokens may be wrong)")

    log_fn(f"Greenhouse: {len(all_jobs)} jobs, {api_calls} calls")
    return all_jobs, api_calls


# ==============================================================================
# SOURCE 4: USAJOBS  (free key — email registration at developer.usajobs.gov)
# ==============================================================================

USAJOBS_BASE = "https://data.usajobs.gov/api/Search"


def scrape_usajobs(api_key, user_agent_email, locations, log_fn, profile: dict):
    """
    Fetch federal government jobs using keywords from the AI-generated profile.
    Requires free API key from developer.usajobs.gov.
    Returns (jobs, api_calls).
    """
    if not api_key or not user_agent_email:
        log_fn("USAJobs: skipped (no API key configured)")
        return [], 0

    keywords = profile.get("usajobs_keywords", FALLBACK_USAJOBS_KEYWORDS)
    if not keywords:
        log_fn("USAJobs: skipped (profile indicates no relevant federal roles)")
        return [], 0

    all_jobs = []
    seen_ids = set()
    api_calls = 0

    headers = {
        "Authorization-Key": api_key,
        "User-Agent": user_agent_email,
        "Host": "data.usajobs.gov",
    }

    log_fn(f"USAJobs: {len(keywords)} keywords...")

    for keyword in keywords:
        try:
            resp = _safe_get(
                USAJOBS_BASE,
                params={
                    "Keyword": keyword,
                    "ResultsPerPage": 25,
                    "SortField": "OpenDate",
                    "SortDirection": "Desc",
                    "GradeLevel": "5;6;7;8;9",
                },
                headers=headers,
                timeout=20,
                source="USAJobs"
            )
            api_calls += 1
            items = resp.json().get("SearchResult", {}).get("SearchResultItems", [])

            new_count = 0
            for item in items:
                match = item.get("MatchedObjectDescriptor", {})
                job_id = "usa_" + str(match.get("PositionID", ""))
                if job_id in seen_ids:
                    continue
                title = (match.get("PositionTitle") or "").strip()
                if not title:
                    continue
                seen_ids.add(job_id)

                org = match.get("OrganizationName", "")
                dept = match.get("DepartmentName", "")

                locs = match.get("PositionLocation", [])
                if isinstance(locs, list) and locs:
                    loc0 = locs[0]
                    location_str = loc0.get("LocationName", "")
                    lat = loc0.get("Latitude")
                    lng = loc0.get("Longitude")
                else:
                    location_str = "United States"
                    lat = lng = None

                telecommute = match.get("UserArea", {}).get("Details", {}).get("Telework", "")
                work_type = "Remote" if "remote" in str(telecommute).lower() else "Onsite"

                rem = match.get("PositionRemuneration", [])
                sal_min = sal_max = None
                sal_display = ""
                if rem:
                    r0 = rem[0]
                    sal_min = _to_int(r0.get("MinimumRange"))
                    sal_max = _to_int(r0.get("MaximumRange"))
                    interval = r0.get("RateIntervalCode", "")
                    if sal_min and sal_max:
                        sal_display = f"${sal_min:,}–${sal_max:,}/{interval.lower() or 'yr'}"

                apply_url = match.get("PositionURI", "")
                all_jobs.append({
                    "job_id": job_id,
                    "title": title,
                    "company": org or dept,
                    "location": location_str,
                    "lat": lat, "lng": lng,
                    "work_type": work_type,
                    "salary_min": sal_min, "salary_max": sal_max, "salary_display": sal_display,
                    "description": match.get("UserArea", {}).get("Details", {}).get("JobSummary", "")[:2500],
                    "apply_url": apply_url,
                    "company_url": apply_url,
                    "source": "USAJobs",
                    "date_posted": match.get("PublicationStartDate", ""),
                })
                new_count += 1

            log_fn(f"  USAJobs [{keyword}]: {new_count}")
            time.sleep(1.0)

        except RateLimitError:
            log_fn("  USAJobs rate limited — stopping")
            break
        except Exception as e:
            log_fn(f"  USAJobs error ({keyword}): {e}")

    log_fn(f"USAJobs: {len(all_jobs)} jobs, {api_calls} calls")
    return all_jobs, api_calls


# ==============================================================================
# SOURCE 5: JSEARCH  (200/month — targeted searches from profile)
# ==============================================================================

def scrape_jsearch_companies(jsearch_key, log_fn, profile: dict):
    """
    Run targeted JSearch queries from the user's AI-generated profile.
    Queries are personalized to the user's resume and target locations.
    """
    queries = profile.get("jsearch_queries", FALLBACK_JSEARCH_QUERIES)
    all_jobs = []
    seen_ids = set()
    api_calls = 0

    headers = {
        "X-RapidAPI-Key": jsearch_key,
        "X-RapidAPI-Host": "jsearch.p.rapidapi.com"
    }

    log_fn(f"JSearch: {len(queries)} targeted queries...")

    for entry in queries:
        try:
            resp = _safe_get(
                "https://jsearch.p.rapidapi.com/search",
                params={"query": entry["query"], "page": "1", "num_pages": "1", "date_posted": "month"},
                headers=headers,
                timeout=20,
                source=f"JSearch/{entry['name']}"
            )
            api_calls += 1
            data = resp.json()

            new_count = 0
            for job in data.get("data", []):
                job_id = job.get("job_id", "")
                if not job_id or job_id in seen_ids:
                    continue
                title = job.get("job_title", "")
                if not is_relevant_title_for_profile(title, profile):
                    continue
                seen_ids.add(job_id)

                sal_min = job.get("job_min_salary")
                sal_max = job.get("job_max_salary")
                sal_period = (job.get("job_salary_period") or "").upper()
                sal_display = ""
                if sal_min and sal_max:
                    sal_display = f"${sal_min:.0f}–${sal_max:.0f}/hr" if sal_period == "HOUR" \
                        else f"${int(sal_min):,}–${int(sal_max):,}/yr"
                elif sal_max:
                    sal_display = f"Up to ${int(sal_max):,}"

                city = job.get("job_city", "")
                state = job.get("job_state", "")
                location_str = ", ".join(filter(None, [city, state])) or job.get("job_country", "")
                apply_url = job.get("job_apply_link", "")

                all_jobs.append({
                    "job_id": job_id,
                    "title": title,
                    "company": job.get("employer_name", "") or entry["name"],
                    "location": location_str,
                    "lat": job.get("job_latitude"), "lng": job.get("job_longitude"),
                    "work_type": "Remote" if job.get("job_is_remote") else "Onsite",
                    "salary_min": _to_int(sal_min), "salary_max": _to_int(sal_max),
                    "salary_display": sal_display,
                    "description": (job.get("job_description") or "")[:2500],
                    "apply_url": apply_url,
                    "company_url": job.get("employer_website", "") or apply_url,
                    "source": "JSearch",
                    "date_posted": job.get("job_posted_at_datetime_utc", ""),
                })
                new_count += 1

            log_fn(f"  JSearch [{entry['name']}]: {new_count}")
            time.sleep(0.5)

        except RateLimitError:
            log_fn("  JSearch rate limited — stopping")
            break
        except Exception as e:
            log_fn(f"  JSearch error ({entry['name']}): {e}")

    log_fn(f"JSearch: {len(all_jobs)} jobs, {api_calls} calls")
    return all_jobs, api_calls


# ==============================================================================
# MAIN ORCHESTRATOR
# ==============================================================================

def scrape_jobs(usajobs_key, usajobs_email, jsearch_key, locations, log_fn,
                skip_jsearch=False, search_profile=None):
    """
    Run all sources in sequence. Merge and deduplicate results.

    search_profile should be the cached AI-generated profile for this user.
    If None, falls back to the generic profile.

    Returns (all_jobs, source_call_counts_dict).
    """
    profile = search_profile or _fallback_profile()

    all_jobs = []
    seen_ids = set()
    call_counts = {"muse": 0, "remotive": 0, "greenhouse": 0, "usajobs": 0, "jsearch": 0}

    def merge(jobs):
        added = 0
        for job in jobs:
            jid = job.get("job_id", "")
            if jid and jid not in seen_ids:
                seen_ids.add(jid)
                all_jobs.append(job)
                added += 1
        return added

    # 1. The Muse
    try:
        jobs, calls = scrape_muse(log_fn, profile)
        call_counts["muse"] = calls
        merge(jobs)
    except Exception as e:
        log_fn(f"Muse source failed: {e}")

    # 2. Remotive
    try:
        jobs, calls = scrape_remotive(log_fn, profile)
        call_counts["remotive"] = calls
        merge(jobs)
    except Exception as e:
        log_fn(f"Remotive source failed: {e}")

    # 3. Greenhouse
    try:
        jobs, calls = scrape_greenhouse(log_fn, profile)
        call_counts["greenhouse"] = calls
        merge(jobs)
    except Exception as e:
        log_fn(f"Greenhouse source failed: {e}")

    # 4. USAJobs (optional)
    if usajobs_key:
        try:
            jobs, calls = scrape_usajobs(usajobs_key, usajobs_email, locations, log_fn, profile)
            call_counts["usajobs"] = calls
            merge(jobs)
        except Exception as e:
            log_fn(f"USAJobs source failed: {e}")
    else:
        log_fn("USAJobs: skipped (no key — free at developer.usajobs.gov)")

    # 5. JSearch — personalized targeted queries
    if jsearch_key and not skip_jsearch:
        try:
            jobs, calls = scrape_jsearch_companies(jsearch_key, log_fn, profile)
            call_counts["jsearch"] = calls
            merge(jobs)
        except Exception as e:
            log_fn(f"JSearch source failed: {e}")
    elif skip_jsearch:
        log_fn("JSearch: skipped (low budget)")
    else:
        log_fn("JSearch: skipped (no key configured)")

    # Cross-source dedup by title+company
    before = len(all_jobs)
    all_jobs = dedup_by_title_company(all_jobs)
    removed = before - len(all_jobs)
    if removed:
        log_fn(f"Cross-source dedup removed {removed} duplicates")

    log_fn(
        f"Total: {len(all_jobs)} unique jobs | "
        f"Muse:{call_counts['muse']} Remotive:{call_counts['remotive']} "
        f"Greenhouse:{call_counts['greenhouse']} USAJobs:{call_counts['usajobs']} "
        f"JSearch:{call_counts['jsearch']}"
    )
    return all_jobs, call_counts


# ==============================================================================
# JSON PARSING  (robust 4-strategy parser for AI responses)
# ==============================================================================

def robust_parse_json_array(text: str, expected_count: int) -> list:
    text = text.strip()
    text_clean = re.sub(r'^```(?:json)?\s*', '', text)
    text_clean = re.sub(r'\s*```$', '', text_clean).strip()

    # Strategy 1: direct array parse
    bracket_match = re.search(r'\[[\s\S]*\]', text_clean)
    if bracket_match:
        try:
            result = json.loads(bracket_match.group())
            if isinstance(result, list):
                return result
        except json.JSONDecodeError:
            pass

    # Strategy 2: fix trailing commas + single quotes
    try:
        fixed = re.sub(r',\s*([}\]])', r'\1', text_clean).replace("'", '"')
        bracket_match2 = re.search(r'\[[\s\S]*\]', fixed)
        if bracket_match2:
            result = json.loads(bracket_match2.group())
            if isinstance(result, list):
                return result
    except Exception:
        pass

    # Strategy 3: extract individual objects and rebuild array
    objects = re.findall(r'\{[^{}]*\}', text_clean, re.DOTALL)
    if objects:
        parsed = []
        for obj_str in objects[:expected_count]:
            try:
                parsed.append(json.loads(obj_str))
            except Exception:
                try:
                    parsed.append(json.loads(re.sub(r',\s*}', '}', obj_str)))
                except Exception:
                    pass
        if parsed:
            return parsed

    raise ValueError(f"Could not parse JSON array. Raw: {text[:200]}")


# ==============================================================================
# AI MATCHING
# ==============================================================================

def match_jobs(jobs, api_key, resume_text, ai_context, api_url, model_name, log_fn):
    """Score jobs against resume. Returns (matched_jobs, ai_calls_used)."""
    matched = []
    ai_calls = 0
    batch_size = 5

    resume_short = resume_text[:2500]
    context_str = f"\nExtra context: {ai_context}" if ai_context else ""

    for i in range(0, len(jobs), batch_size):
        batch = jobs[i:i + batch_size]
        batch_num = i // batch_size + 1
        total_batches = (len(jobs) + batch_size - 1) // batch_size
        log_fn(f"AI matching batch {batch_num}/{total_batches} ({len(batch)} jobs)...")

        jobs_text = ""
        for j, job in enumerate(batch):
            jobs_text += (
                f"\nJob {j + 1}: {job['title']} @ {job['company']}\n"
                f"Location: {job['location']} | Type: {job['work_type']} | Salary: {job['salary_display'] or 'unlisted'}\n"
                f"Desc: {job['description'][:400]}\n---"
            )

        prompt = (
            f"You are a technical recruiter evaluating job fit.\n\n"
            f"CANDIDATE RESUME:\n{resume_short}{context_str}\n\n"
            f"JOBS TO SCORE:\n{jobs_text}\n\n"
            f"Scoring: 70-100=strong match, 40-69=worth applying, 0-39=poor fit.\n"
            f"Boost entry-level/new-grad/associate roles. "
            f"Correct work_type to Remote/Hybrid/Onsite based on description.\n\n"
            f"YOU MUST respond with ONLY a JSON array of exactly {len(batch)} objects:\n"
            f'[{{"score":85,"reasons":"Strong Python match. Entry-level.","work_type":"Remote"}},...]\n'
            f"No prose, no markdown, ONLY the JSON array."
        )

        success = False
        for attempt in range(3):
            try:
                resp = requests.post(
                    api_url,
                    headers={"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"},
                    json={
                        "model": model_name,
                        "messages": [
                            {"role": "system", "content": "You are a JSON-only API. Respond only with valid JSON arrays."},
                            {"role": "user", "content": prompt}
                        ],
                        "stream": False
                    },
                    timeout=120
                )
                ai_calls += 1
                resp.raise_for_status()
                content = resp.json()["choices"][0]["message"]["content"].strip()
                ratings = robust_parse_json_array(content, len(batch))

                for j, job in enumerate(batch):
                    if j < len(ratings):
                        r = ratings[j]
                        job["match_score"] = max(0, min(100, int(r.get("score", 50))))
                        job["match_reasons"] = str(r.get("reasons", "")).strip()
                        job["work_type"] = r.get("work_type", job["work_type"])
                    else:
                        job["match_score"] = -1
                        job["match_reasons"] = "Score unavailable (partial response)"
                    matched.append(job)
                success = True
                break

            except Exception as e:
                log_fn(f"  Attempt {attempt + 1}/3 failed: {e}")
                if attempt < 2:
                    time.sleep(3)

        if not success:
            log_fn(f"  Batch {batch_num} failed all retries — marking unscored")
            for job in batch:
                job["match_score"] = -1
                job["match_reasons"] = "AI matching failed — use Rescore to retry"
                matched.append(job)

        time.sleep(1)

    log_fn(f"AI matching complete: {len(matched)} jobs, {ai_calls} AI calls")
    return matched, ai_calls


# ==============================================================================
# HELPERS
# ==============================================================================

def _to_int(val):
    try:
        return int(float(val))
    except (TypeError, ValueError):
        return None


def _parse_salary_range(sal_str: str):
    """Extract min/max salary from strings like '$40,000 - $60,000'."""
    if not sal_str:
        return None, None
    nums = re.findall(r'[\d,]+', sal_str.replace(',', ''))
    nums = [int(n) for n in nums if n.isdigit() and int(n) > 1000]
    if len(nums) >= 2:
        return min(nums), max(nums)
    elif len(nums) == 1:
        return nums[0], nums[0]
    return None, None
